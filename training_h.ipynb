{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch pandas numpy h5py tqdm scikit-learn tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18789/1373997312.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from src.dataset import ProteinDataset\n",
    "from src.utils import train_model, test_model\n",
    "import torch\n",
    "from src.model import ChemicalShiftsPredictor, ChemicalShiftsPredictorAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "csv_file = 'data/strict.csv'\n",
    "prott5_file = 'data/embeddings/unfiltered_all_prott5.h5'\n",
    "prott5_res_file = 'data/embeddings/unfiltered_all_prott5_res.h5'\n",
    "prostt5_file = 'data/embeddings/prostt5.h5'\n",
    "esm_file = 'data/embeddings/unfiltered_all_esm2_3b.h5'\n",
    "esm_res_file = 'data/embeddings/unfiltered_all_esm2_3b_res.h5'\n",
    "chemical_shifts_df = pd.read_csv(csv_file)\n",
    "#chemical_shifts_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = []\n",
    "with open(\"pdb_matched/final_test_ids.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_ids.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemical_shifts_df = chemical_shifts_df[~chemical_shifts_df['ID'].isin(test_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_applied = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_columns = ['C', 'CA', 'CB', 'HA', 'H', 'N', 'HB']\n",
    "target_columns = ['H']\n",
    "chemical_shifts_df.dropna(inplace=True, subset=target_columns)\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(chemical_shifts_df, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# training with whole dataset here\n",
    "train_targets = chemical_shifts_df[target_columns]\n",
    "scaler.fit(train_targets)\n",
    "\n",
    "joblib.dump(scaler, 'scaler_h.joblib')\n",
    "\n",
    "# Apply normalization to the training targets\n",
    "#train_df[target_columns] = scaler.transform(train_targets)\n",
    "\n",
    "# Apply the same normalization to validation and test sets\n",
    "# val_df[target_columns] = scaler.transform(val_df[target_columns])\n",
    "# test_df[target_columns] = scaler.transform(test_df[target_columns])\n",
    "\n",
    "# Create datasets\n",
    "if not scaler_applied:\n",
    "    chemical_shifts_df[target_columns] = scaler.transform(chemical_shifts_df[target_columns])\n",
    "    train_df[target_columns] = scaler.transform(train_df[target_columns])\n",
    "    val_df[target_columns] = scaler.transform(val_df[target_columns])\n",
    "    scaler_applied = True\n",
    "    \n",
    "    \n",
    "train_dataset = ProteinDataset(target_columns, chemical_shifts_df, prott5_file, prott5_res_file, prostt5_file, esm_res_file, esm_file)\n",
    "val_dataset = ProteinDataset(target_columns, val_df, prott5_file, prott5_res_file, prostt5_file, esm_res_file, esm_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 1e-5\n",
    "patience = 10\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "\n",
    "use_prostt5 = True\n",
    "use_protein_mean = True\n",
    "use_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ChemicalShiftsPredictor(use_prostt5=use_prostt5, use_protein_mean=use_protein_mean, use_attention=use_attention)\n",
    "#model.load_state_dict(torch.load('Full_1e-4.pth'))\n",
    "\n",
    "#model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChemicalShiftsPredictor(\n",
      "  (light_attention): LightAttention(\n",
      "    (feature_convolution): Conv1d(1024, 1024, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (attention_convolution): Conv1d(1024, 1024, kernel_size=(9,), stride=(1,), padding=(4,))\n",
      "    (softmax): Softmax(dim=-1)\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=7680, out_features=7680, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=7680, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|██████████| 1413/1413 [19:05<00:00,  1.23batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.7511, Validation Loss: 0.4135\n",
      "Epoch 1, Train RMSE: 0.5075, Validation RMSE: 0.4428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 1413/1413 [20:52<00:00,  1.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.5650, Validation Loss: 0.3802\n",
      "Epoch 2, Train RMSE: 0.4520, Validation RMSE: 0.4248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 1413/1413 [21:06<00:00,  1.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.5335, Validation Loss: 0.3393\n",
      "Epoch 3, Train RMSE: 0.4348, Validation RMSE: 0.4008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 1413/1413 [20:42<00:00,  1.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.4841, Validation Loss: 0.4322\n",
      "Epoch 4, Train RMSE: 0.4075, Validation RMSE: 0.4528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 1413/1413 [20:31<00:00,  1.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 1.1505, Validation Loss: 0.4899\n",
      "Epoch 5, Train RMSE: 0.5582, Validation RMSE: 0.4819\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_model(train_dataset, val_dataset, learning_rate=learning_rate, num_epochs=num_epochs, weight_decay=weight_decay,\n",
    "                            patience=patience, batch_size=batch_size, use_prostt5=use_prostt5 , use_protein_mean=use_protein_mean, scaler=scaler, use_attention=use_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(trained_model.state_dict(), 'Full_1e-4_H.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_to_one = {\n",
    "    'ALA': 'A', 'ARG': 'R', 'ASN': 'N', 'ASP': 'D', 'CYS': 'C', \n",
    "    'GLU': 'E', 'GLN': 'Q', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I', \n",
    "    'LEU': 'L', 'LYS': 'K', 'MET': 'M', 'PHE': 'F', 'PRO': 'P', \n",
    "    'SER': 'S', 'THR': 'T', 'TRP': 'W', 'TYR': 'Y', 'VAL': 'V',\n",
    "}\n",
    "\n",
    "def process_ucb_output(dataframe, id):\n",
    "    new_columns = {}\n",
    "    new_columns[\"ID\"] = id\n",
    "    new_columns['seq_index'] = dataframe[\"RESNUM\"] - min(dataframe[\"RESNUM\"]) + 1\n",
    "    new_columns['seq'] = [three_to_one[res] for res in dataframe[\"RESNAME\"]]\n",
    "    new_columns['H'] = dataframe[\"H_UCBShift\"]\n",
    "    new_columns['N'] = dataframe[\"N_UCBShift\"]\n",
    "    new_df = pd.DataFrame(new_columns)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_ucb_output(ucb_predictions['34695_1_1_1'], '34695_1_1_1')\n",
    "\n",
    "# process all ucb predictions, make single dataframe\n",
    "all_ucb_predictions = []\n",
    "for id, dataframe in ucb_predictions.items():\n",
    "    all_ucb_predictions.append(process_ucb_output(dataframe, id))\n",
    "    \n",
    "all_ucb_predictions = pd.concat(all_ucb_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ucb_predictions.to_csv('all_ucb_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "trained_model = train_model(train_dataset, val_dataset, learning_rate=learning_rate, num_epochs=50, weight_decay=weight_decay, patience=patience, batch_size=2048, use_prostt5=True, use_protein_mean=True, use_esm2=False)\n",
    "test_model(trained_model, test_dataset, batch_size=batch_size, use_prostt5=True, use_protein_mean=True, use_esm2=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
